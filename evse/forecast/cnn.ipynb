{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "from tensorflow.keras import layers as l\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/4209770250.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self, path: str) -> None:    \n",
    "        demand_path = path + \"/Demand_history.csv\"\n",
    "        existingEV_path = path + \"/existing_EV_infrastructure_2018.csv\"\n",
    "        self.df_orig = pd.read_csv(demand_path)\n",
    "        self.df_trans = self.df_orig\n",
    "        self.years_window = 3\n",
    "        self.y_cols = [f\"n-{y}\" for y in range(1, self.years_window + 1)]\n",
    "        self.dem_scaler = MinMaxScaler()\n",
    "        self.coord_scaler = MinMaxScaler()\n",
    "        self.seq_len = 8\n",
    "        \n",
    "    def clean(self):\n",
    "        self.df_orig.loc[(self.df_orig != 0).any(1)]\n",
    "        print(self.df_orig)\n",
    "        \n",
    "    def process(self):\n",
    "        all_dem = self.df_trans[\n",
    "            self.df_trans.columns[self.df_trans.columns.str.startswith('20')]].stack()\n",
    "        self.dem_scaler.fit(all_dem.values.reshape(-1, 1))\n",
    "        self.coord_scaler.fit(self.df_trans.loc[:, self.df_trans.columns.str.contains('coord')])\n",
    "        \n",
    "        self.x_proc = pd.DataFrame(columns=[\"x_coordinate\", \"y_coordinate\", *self.y_cols])\n",
    "        self.y_proc = pd.Series(dtype=np.float64)\n",
    "        for y in self.df_trans.columns[self.df_trans.columns.str.startswith('20')]:\n",
    "            y = int(y)\n",
    "            if y < 2010 + self.years_window:\n",
    "                continue\n",
    "            y_cols = [f\"{y - i}\" for i in range(1, self.years_window + 1)]\n",
    "            stack_x = self.df_trans.loc[:, self.df_trans.columns.isin([\"x_coordinate\", \"y_coordinate\", *y_cols])]\n",
    "            stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
    "            for col in y_cols:\n",
    "                stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
    "            y_dict = {f\"{y - i}\": f\"n-{i}\" for i in range(1, self.years_window + 1)}\n",
    "            stack_x = stack_x.rename(columns=y_dict)\n",
    "            stack_y = self.df_trans.loc[:, f\"{y}\"]\n",
    "            stack_y = pd.Series(self.dem_scaler.transform(stack_y.values.reshape(-1, 1)).flatten())\n",
    "            self.x_proc = pd.concat([self.x_proc, stack_x], axis=0, ignore_index=True)\n",
    "            self.y_proc = pd.concat([self.y_proc, stack_y], axis=0, ignore_index=True)\n",
    "        \n",
    "        self.x_list = [self.x_proc.iloc[i * self.seq_len: (i + 1) * self.seq_len, :] for i in range(int(self.x_proc.shape[0] / self.seq_len))]\n",
    "        self.y_list = [self.y_proc.iloc[i * self.seq_len: (i + 1) * self.seq_len] for i in range(int(self.x_proc.shape[0] / self.seq_len))]\n",
    "        # self.x_list = np.array(self.x_list)\n",
    "        # self.y_list = np.array(self.y_list)\n",
    "        self.train_idx = np.random.choice(len(self.x_list), int(len(self.x_list) * 0.8), replace=False)\n",
    "        self.test_idx = [i for i in range(len(self.x_list)) if i not in self.train_idx]   \n",
    "        \n",
    "        \n",
    "    def datagen(self, kind):\n",
    "        if kind == 'train':\n",
    "            idxs = self.train_idx\n",
    "        elif kind == 'valid':\n",
    "            idxs = self.test_idx\n",
    "            \n",
    "        x = [self.x_list[i].values for i in idxs] \n",
    "        y = [self.y_list[i].values for i in idxs] \n",
    "        return np.array(x), np.array(y)\n",
    "                    \n",
    "    def addYearDemandfromForecast(self, year: int, predicted: np.array) -> None:\n",
    "        predicted = predicted.flatten().reshape(-1, 1)\n",
    "        predicted = self.dem_scaler.inverse_transform(predicted)\n",
    "        self.df_trans[f\"{year}\"] = abs(predicted)        \n",
    "                  \n",
    "path = \"data\"\n",
    "data = Data(path)\n",
    "data.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, data: Data) -> None:\n",
    "        self.data = data\n",
    "        self.batch_size = 16\n",
    "        self.filter_size = 256\n",
    "        self.epochs = 1000\n",
    "    \n",
    "    def createModel(self):\n",
    "        inputs = l.Input(shape=(data.seq_len, data.years_window + 2))\n",
    "        cnn1 = l.Conv1D(self.filter_size, kernel_size=1, activation=\"relu\", padding=\"valid\")(inputs)\n",
    "        cnn2 = l.Conv1D(self.filter_size, kernel_size=2, activation=\"relu\", padding=\"valid\")(cnn1)\n",
    "        cnn3 = l.Conv1D(self.filter_size, kernel_size=2, activation=\"relu\", padding=\"valid\")(cnn2)\n",
    "        mp1 = l.MaxPool1D(pool_size=2)(cnn3)\n",
    "        cnn4 = l.Conv1D(self.filter_size, kernel_size=3, activation=\"relu\", padding=\"valid\")(mp1)\n",
    "        mp2 = l.MaxPool1D(pool_size=1)(cnn4)\n",
    "        fl = l.Flatten()(mp2)\n",
    "        do = l.Dropout(0.1)(fl)\n",
    "        outputs = l.Dense(data.seq_len, activation=\"relu\")(do)\n",
    "        self.model = k.Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer=k.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "        self.model.summary()\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        \n",
    "        callbacks = []\n",
    "        # mp = \"/mod/checkpoint\"\n",
    "        # cbcp = k.ModelCheckpoint(mp,\n",
    "        #             monitor='val_mse', mode=\"auto\", verbose=0,\n",
    "        #             save_best_only=True, save_weights_only=True, save_freq=\"epoch\")\n",
    "        cbes = k.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=100,\n",
    "            verbose=0,\n",
    "            mode=\"auto\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        callbacks.append(cbes)\n",
    "        \n",
    "        rlr = k.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=20,\n",
    "            min_lr=0,\n",
    "            min_delta=0.00008)\n",
    "        \n",
    "        callbacks.append(rlr)\n",
    "        \n",
    "        \n",
    "        x_t, y_t = data.datagen('train')\n",
    "        x_v, y_v = data.datagen('valid')\n",
    "        \n",
    "        self.history = self.model.fit(x_t, y_t, validation_data=(x_v, y_v),\n",
    "                                      epochs=self.epochs, verbose=1, callbacks=callbacks)\n",
    "        # self.model.load_weights(mp) \n",
    "        \n",
    "        pred = self.model.predict(x_t)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_t, pred))\n",
    "        train_mae = mean_absolute_error(y_t, pred)   \n",
    "        train_r2 = r2_score(y_t, pred) \n",
    "        pred = self.model.predict(x_v)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_v, pred))\n",
    "        test_mae = mean_absolute_error(y_v, pred)   \n",
    "        test_r2 = r2_score(y_v, pred) \n",
    "           \n",
    "        \n",
    "        print(f\"training: rmse={train_rmse}, mae={train_mae}, r2={train_r2}\")   \n",
    "        print(f\"test: rmse={test_rmse}, mae={test_mae}, r2={test_r2}\")                        \n",
    "    \n",
    "    def predict(self, year: int) -> pd.Series:\n",
    "        y_cols = [f\"{year - i}\" for i in range(1, data.years_window + 1)]\n",
    "        x_forecast = \\\n",
    "            data.df_trans.loc[:,\n",
    "                              data.df_trans.columns.isin([\"x_coordinate\", \"y_coordinate\", *y_cols])]        \n",
    "        x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = data.coord_scaler.transform(x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
    "        for col in y_cols:  \n",
    "            x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
    "        x_list = [x_forecast.iloc[i * data.seq_len: (i + 1) * data.seq_len, :] for i in range(int(x_forecast.shape[0] / data.seq_len))]\n",
    "        x_forecast = np.array(x_list)\n",
    "        print(x_forecast.shape)\n",
    "        return self.model.predict(x_forecast)\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        params['n_estimators'] = int(params['n_estimators'])\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "        self.model.set_params(**params)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 8, 5)]            0         \n",
      "                                                                 \n",
      " conv1d_32 (Conv1D)          (None, 8, 256)            1536      \n",
      "                                                                 \n",
      " conv1d_33 (Conv1D)          (None, 7, 256)            131328    \n",
      "                                                                 \n",
      " conv1d_34 (Conv1D)          (None, 6, 256)            131328    \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPoolin  (None, 3, 256)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 1, 256)            196864    \n",
      "                                                                 \n",
      " max_pooling1d_17 (MaxPoolin  (None, 1, 256)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 463,112\n",
      "Trainable params: 463,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 09:54:45.224465: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - ETA: 0s - loss: 0.0093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 09:54:46.491652: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 15ms/step - loss: 0.0093 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0037 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0028 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0023 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0021 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0019 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0017 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0017 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0016 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0015 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0015 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0014 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0013 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0013 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0014 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0012 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0012 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0012 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0012 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 9.5781e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 9.5164e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 9.4632e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 9.0600e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 8.7863e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 9.0264e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 8.9816e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 8.4904e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 9.2398e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.5984e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.7728e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 8.4273e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.4900e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.9298e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.6659e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.2903e-04 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.7162e-04 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.9935e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.8051e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.9392e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.7455e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.2158e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 7.0558e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.3980e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.2777e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.9536e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.2798e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.9774e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.4090e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 6.4695e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.9511e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.8515e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.6254e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.3380e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.2489e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 5.3303e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 4.3042e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 67/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.8120e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 68/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.8517e-04 - val_loss: 9.7032e-04 - lr: 5.0000e-04\n",
      "Epoch 69/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.6846e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 70/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.6939e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 71/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.6453e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 72/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.8268e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 73/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.4956e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 74/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.6039e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 75/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.6180e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 76/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.3897e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 77/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.3992e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 78/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.4141e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 79/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.2324e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 80/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.2354e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 81/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.1693e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 82/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.4448e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 83/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.1603e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 84/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.1454e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 85/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 3.1237e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 86/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.7097e-04 - val_loss: 0.0012 - lr: 2.5000e-04\n",
      "Epoch 87/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.5637e-04 - val_loss: 0.0012 - lr: 2.5000e-04\n",
      "Epoch 88/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.5042e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 89/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.5768e-04 - val_loss: 0.0012 - lr: 2.5000e-04\n",
      "Epoch 90/1000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 2.4864e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 91/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.6215e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 92/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.5372e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 93/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.4462e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 94/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.5797e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 95/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.4434e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 96/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.4445e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 97/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.4474e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 98/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.4186e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 99/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.2671e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 100/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.2742e-04 - val_loss: 0.0012 - lr: 2.5000e-04\n",
      "Epoch 101/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.3604e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 102/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.2978e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 103/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.2434e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 104/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.3426e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 105/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.3911e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 106/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.0866e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 107/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.0692e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 108/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.0297e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 109/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.0581e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 110/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9928e-04 - val_loss: 0.0012 - lr: 1.2500e-04\n",
      "Epoch 111/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9847e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 112/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9636e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 113/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9947e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 114/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9196e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 115/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.0298e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 116/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9600e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 117/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 2.0070e-04 - val_loss: 0.0012 - lr: 1.2500e-04\n",
      "Epoch 118/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9677e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 119/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9636e-04 - val_loss: 0.0012 - lr: 1.2500e-04\n",
      "Epoch 120/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9431e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 121/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.8823e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 122/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9251e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 123/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9314e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 124/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.8972e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 125/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.9902e-04 - val_loss: 0.0011 - lr: 1.2500e-04\n",
      "Epoch 126/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.8095e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 127/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.8084e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 128/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.8134e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 129/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7796e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 130/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7752e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 131/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7739e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 132/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7665e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 133/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7584e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 134/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7710e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 135/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7338e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 136/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7584e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 137/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7701e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 138/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7663e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 139/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7506e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 140/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7413e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 141/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7122e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 142/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7395e-04 - val_loss: 0.0012 - lr: 6.2500e-05\n",
      "Epoch 143/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7142e-04 - val_loss: 0.0012 - lr: 6.2500e-05\n",
      "Epoch 144/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7328e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 145/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6969e-04 - val_loss: 0.0011 - lr: 6.2500e-05\n",
      "Epoch 146/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7181e-04 - val_loss: 0.0012 - lr: 3.1250e-05\n",
      "Epoch 147/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6423e-04 - val_loss: 0.0012 - lr: 3.1250e-05\n",
      "Epoch 148/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6693e-04 - val_loss: 0.0012 - lr: 3.1250e-05\n",
      "Epoch 149/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6685e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 150/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.7076e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 151/1000\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 1.6369e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 152/1000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 1.6780e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 153/1000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 1.6506e-04 - val_loss: 0.0012 - lr: 3.1250e-05\n",
      "Epoch 154/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6256e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 155/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6243e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 156/1000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 1.6232e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 157/1000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 1.5813e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 158/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6279e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 159/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6619e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 160/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.5764e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 161/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6134e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 162/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6215e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 163/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.5989e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 164/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6432e-04 - val_loss: 0.0012 - lr: 3.1250e-05\n",
      "Epoch 165/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6042e-04 - val_loss: 0.0011 - lr: 3.1250e-05\n",
      "Epoch 166/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.6037e-04 - val_loss: 0.0011 - lr: 1.5625e-05\n",
      "Epoch 167/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.5785e-04 - val_loss: 0.0011 - lr: 1.5625e-05\n",
      "Epoch 168/1000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.5827e-04 - val_loss: 0.0011 - lr: 1.5625e-05\n",
      "37/77 [=============>................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 09:56:54.128056: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 3ms/step\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "training: rmse=0.016929732617784625, mae=0.010924291878664132, r2=0.9919098140501384\n",
      "test: rmse=0.031149984635930588, mae=0.01835250602002895, r2=0.9740580165944599\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 8, 5)\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = data.coord_scaler.transform(x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "predicted_2019 = model.predict(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.addYearDemandfromForecast(2019, predicted_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_point_index</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.352242</td>\n",
       "      <td>0.667932</td>\n",
       "      <td>0.958593</td>\n",
       "      <td>2.911901</td>\n",
       "      <td>4.338274</td>\n",
       "      <td>6.561995</td>\n",
       "      <td>8.454417</td>\n",
       "      <td>10.595324</td>\n",
       "      <td>13.119572</td>\n",
       "      <td>15.281493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.325940</td>\n",
       "      <td>0.591964</td>\n",
       "      <td>0.862652</td>\n",
       "      <td>2.589068</td>\n",
       "      <td>4.196034</td>\n",
       "      <td>5.745551</td>\n",
       "      <td>8.753195</td>\n",
       "      <td>11.126995</td>\n",
       "      <td>12.020091</td>\n",
       "      <td>17.997620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.373752</td>\n",
       "      <td>0.591890</td>\n",
       "      <td>0.969733</td>\n",
       "      <td>2.641432</td>\n",
       "      <td>3.541772</td>\n",
       "      <td>5.469161</td>\n",
       "      <td>8.414627</td>\n",
       "      <td>10.115336</td>\n",
       "      <td>14.018254</td>\n",
       "      <td>17.831633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.420686</td>\n",
       "      <td>0.584055</td>\n",
       "      <td>0.906547</td>\n",
       "      <td>2.378577</td>\n",
       "      <td>3.888121</td>\n",
       "      <td>5.846089</td>\n",
       "      <td>9.083868</td>\n",
       "      <td>12.424885</td>\n",
       "      <td>15.012302</td>\n",
       "      <td>20.571795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.475621</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.981544</td>\n",
       "      <td>2.665400</td>\n",
       "      <td>4.218711</td>\n",
       "      <td>6.776609</td>\n",
       "      <td>8.851107</td>\n",
       "      <td>11.731131</td>\n",
       "      <td>16.355563</td>\n",
       "      <td>19.273750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>4091</td>\n",
       "      <td>59.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.334565</td>\n",
       "      <td>0.556055</td>\n",
       "      <td>1.373291</td>\n",
       "      <td>1.837586</td>\n",
       "      <td>2.517146</td>\n",
       "      <td>3.352280</td>\n",
       "      <td>4.149888</td>\n",
       "      <td>5.426193</td>\n",
       "      <td>6.251414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4092</td>\n",
       "      <td>60.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.041716</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.131291</td>\n",
       "      <td>0.386540</td>\n",
       "      <td>0.755846</td>\n",
       "      <td>0.941116</td>\n",
       "      <td>1.107797</td>\n",
       "      <td>1.309479</td>\n",
       "      <td>2.057450</td>\n",
       "      <td>4.552162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>4093</td>\n",
       "      <td>61.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.100895</td>\n",
       "      <td>0.180352</td>\n",
       "      <td>0.296299</td>\n",
       "      <td>0.705373</td>\n",
       "      <td>1.300220</td>\n",
       "      <td>1.608609</td>\n",
       "      <td>1.822806</td>\n",
       "      <td>2.333681</td>\n",
       "      <td>3.218519</td>\n",
       "      <td>2.781508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>4094</td>\n",
       "      <td>62.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.155353</td>\n",
       "      <td>0.290825</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>1.516066</td>\n",
       "      <td>2.399426</td>\n",
       "      <td>2.719197</td>\n",
       "      <td>4.494515</td>\n",
       "      <td>6.096858</td>\n",
       "      <td>6.262574</td>\n",
       "      <td>9.360272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>4095</td>\n",
       "      <td>63.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.340185</td>\n",
       "      <td>0.749491</td>\n",
       "      <td>1.904285</td>\n",
       "      <td>2.775772</td>\n",
       "      <td>3.404641</td>\n",
       "      <td>4.574922</td>\n",
       "      <td>6.301078</td>\n",
       "      <td>6.860939</td>\n",
       "      <td>12.595498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      demand_point_index  x_coordinate  y_coordinate      2010      2011  \\\n",
       "0                      0           0.5           0.5  0.352242  0.667932   \n",
       "1                      1           1.5           0.5  0.325940  0.591964   \n",
       "2                      2           2.5           0.5  0.373752  0.591890   \n",
       "3                      3           3.5           0.5  0.420686  0.584055   \n",
       "4                      4           4.5           0.5  0.475621  0.647940   \n",
       "...                  ...           ...           ...       ...       ...   \n",
       "4091                4091          59.5          63.5  0.171015  0.334565   \n",
       "4092                4092          60.5          63.5  0.041716  0.061741   \n",
       "4093                4093          61.5          63.5  0.100895  0.180352   \n",
       "4094                4094          62.5          63.5  0.155353  0.290825   \n",
       "4095                4095          63.5          63.5  0.209372  0.340185   \n",
       "\n",
       "          2012      2013      2014      2015      2016       2017       2018  \\\n",
       "0     0.958593  2.911901  4.338274  6.561995  8.454417  10.595324  13.119572   \n",
       "1     0.862652  2.589068  4.196034  5.745551  8.753195  11.126995  12.020091   \n",
       "2     0.969733  2.641432  3.541772  5.469161  8.414627  10.115336  14.018254   \n",
       "3     0.906547  2.378577  3.888121  5.846089  9.083868  12.424885  15.012302   \n",
       "4     0.981544  2.665400  4.218711  6.776609  8.851107  11.731131  16.355563   \n",
       "...        ...       ...       ...       ...       ...        ...        ...   \n",
       "4091  0.556055  1.373291  1.837586  2.517146  3.352280   4.149888   5.426193   \n",
       "4092  0.131291  0.386540  0.755846  0.941116  1.107797   1.309479   2.057450   \n",
       "4093  0.296299  0.705373  1.300220  1.608609  1.822806   2.333681   3.218519   \n",
       "4094  0.557803  1.516066  2.399426  2.719197  4.494515   6.096858   6.262574   \n",
       "4095  0.749491  1.904285  2.775772  3.404641  4.574922   6.301078   6.860939   \n",
       "\n",
       "           2019  \n",
       "0     15.281493  \n",
       "1     17.997620  \n",
       "2     17.831633  \n",
       "3     20.571795  \n",
       "4     19.273750  \n",
       "...         ...  \n",
       "4091   6.251414  \n",
       "4092   4.552162  \n",
       "4093   2.781508  \n",
       "4094   9.360272  \n",
       "4095  12.595498  \n",
       "\n",
       "[4096 rows x 13 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 8, 5)\n",
      "16/16 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = data.coord_scaler.transform(x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_2024/3812670843.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "predicted_2020 = model.predict(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.addYearDemandfromForecast(2020, predicted_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_point_index</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.352242</td>\n",
       "      <td>0.667932</td>\n",
       "      <td>0.958593</td>\n",
       "      <td>2.911901</td>\n",
       "      <td>4.338274</td>\n",
       "      <td>6.561995</td>\n",
       "      <td>8.454417</td>\n",
       "      <td>10.595324</td>\n",
       "      <td>13.119572</td>\n",
       "      <td>15.281493</td>\n",
       "      <td>18.934301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.325940</td>\n",
       "      <td>0.591964</td>\n",
       "      <td>0.862652</td>\n",
       "      <td>2.589068</td>\n",
       "      <td>4.196034</td>\n",
       "      <td>5.745551</td>\n",
       "      <td>8.753195</td>\n",
       "      <td>11.126995</td>\n",
       "      <td>12.020091</td>\n",
       "      <td>17.997620</td>\n",
       "      <td>22.817757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.373752</td>\n",
       "      <td>0.591890</td>\n",
       "      <td>0.969733</td>\n",
       "      <td>2.641432</td>\n",
       "      <td>3.541772</td>\n",
       "      <td>5.469161</td>\n",
       "      <td>8.414627</td>\n",
       "      <td>10.115336</td>\n",
       "      <td>14.018254</td>\n",
       "      <td>17.831633</td>\n",
       "      <td>21.819876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.420686</td>\n",
       "      <td>0.584055</td>\n",
       "      <td>0.906547</td>\n",
       "      <td>2.378577</td>\n",
       "      <td>3.888121</td>\n",
       "      <td>5.846089</td>\n",
       "      <td>9.083868</td>\n",
       "      <td>12.424885</td>\n",
       "      <td>15.012302</td>\n",
       "      <td>20.571795</td>\n",
       "      <td>24.794550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.475621</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.981544</td>\n",
       "      <td>2.665400</td>\n",
       "      <td>4.218711</td>\n",
       "      <td>6.776609</td>\n",
       "      <td>8.851107</td>\n",
       "      <td>11.731131</td>\n",
       "      <td>16.355563</td>\n",
       "      <td>19.273750</td>\n",
       "      <td>25.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>4091</td>\n",
       "      <td>59.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.334565</td>\n",
       "      <td>0.556055</td>\n",
       "      <td>1.373291</td>\n",
       "      <td>1.837586</td>\n",
       "      <td>2.517146</td>\n",
       "      <td>3.352280</td>\n",
       "      <td>4.149888</td>\n",
       "      <td>5.426193</td>\n",
       "      <td>6.251414</td>\n",
       "      <td>7.296557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4092</td>\n",
       "      <td>60.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.041716</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.131291</td>\n",
       "      <td>0.386540</td>\n",
       "      <td>0.755846</td>\n",
       "      <td>0.941116</td>\n",
       "      <td>1.107797</td>\n",
       "      <td>1.309479</td>\n",
       "      <td>2.057450</td>\n",
       "      <td>4.552162</td>\n",
       "      <td>6.137774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>4093</td>\n",
       "      <td>61.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.100895</td>\n",
       "      <td>0.180352</td>\n",
       "      <td>0.296299</td>\n",
       "      <td>0.705373</td>\n",
       "      <td>1.300220</td>\n",
       "      <td>1.608609</td>\n",
       "      <td>1.822806</td>\n",
       "      <td>2.333681</td>\n",
       "      <td>3.218519</td>\n",
       "      <td>2.781508</td>\n",
       "      <td>3.693556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>4094</td>\n",
       "      <td>62.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.155353</td>\n",
       "      <td>0.290825</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>1.516066</td>\n",
       "      <td>2.399426</td>\n",
       "      <td>2.719197</td>\n",
       "      <td>4.494515</td>\n",
       "      <td>6.096858</td>\n",
       "      <td>6.262574</td>\n",
       "      <td>9.360272</td>\n",
       "      <td>11.166464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>4095</td>\n",
       "      <td>63.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.340185</td>\n",
       "      <td>0.749491</td>\n",
       "      <td>1.904285</td>\n",
       "      <td>2.775772</td>\n",
       "      <td>3.404641</td>\n",
       "      <td>4.574922</td>\n",
       "      <td>6.301078</td>\n",
       "      <td>6.860939</td>\n",
       "      <td>12.595498</td>\n",
       "      <td>14.988535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      demand_point_index  x_coordinate  y_coordinate      2010      2011  \\\n",
       "0                      0           0.5           0.5  0.352242  0.667932   \n",
       "1                      1           1.5           0.5  0.325940  0.591964   \n",
       "2                      2           2.5           0.5  0.373752  0.591890   \n",
       "3                      3           3.5           0.5  0.420686  0.584055   \n",
       "4                      4           4.5           0.5  0.475621  0.647940   \n",
       "...                  ...           ...           ...       ...       ...   \n",
       "4091                4091          59.5          63.5  0.171015  0.334565   \n",
       "4092                4092          60.5          63.5  0.041716  0.061741   \n",
       "4093                4093          61.5          63.5  0.100895  0.180352   \n",
       "4094                4094          62.5          63.5  0.155353  0.290825   \n",
       "4095                4095          63.5          63.5  0.209372  0.340185   \n",
       "\n",
       "          2012      2013      2014      2015      2016       2017       2018  \\\n",
       "0     0.958593  2.911901  4.338274  6.561995  8.454417  10.595324  13.119572   \n",
       "1     0.862652  2.589068  4.196034  5.745551  8.753195  11.126995  12.020091   \n",
       "2     0.969733  2.641432  3.541772  5.469161  8.414627  10.115336  14.018254   \n",
       "3     0.906547  2.378577  3.888121  5.846089  9.083868  12.424885  15.012302   \n",
       "4     0.981544  2.665400  4.218711  6.776609  8.851107  11.731131  16.355563   \n",
       "...        ...       ...       ...       ...       ...        ...        ...   \n",
       "4091  0.556055  1.373291  1.837586  2.517146  3.352280   4.149888   5.426193   \n",
       "4092  0.131291  0.386540  0.755846  0.941116  1.107797   1.309479   2.057450   \n",
       "4093  0.296299  0.705373  1.300220  1.608609  1.822806   2.333681   3.218519   \n",
       "4094  0.557803  1.516066  2.399426  2.719197  4.494515   6.096858   6.262574   \n",
       "4095  0.749491  1.904285  2.775772  3.404641  4.574922   6.301078   6.860939   \n",
       "\n",
       "           2019       2020  \n",
       "0     15.281493  18.934301  \n",
       "1     17.997620  22.817757  \n",
       "2     17.831633  21.819876  \n",
       "3     20.571795  24.794550  \n",
       "4     19.273750  25.423000  \n",
       "...         ...        ...  \n",
       "4091   6.251414   7.296557  \n",
       "4092   4.552162   6.137774  \n",
       "4093   2.781508   3.693556  \n",
       "4094   9.360272  11.166464  \n",
       "4095  12.595498  14.988535  \n",
       "\n",
       "[4096 rows x 14 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.df_trans.to_csv(\"data/forecast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f414c7ff5a53ed42f5544c97695d2b1ab51d06b1c0aa8a68a2484fa0fe1f8ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
