{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "from tensorflow.keras import layers as l\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/402615752.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "class Data:\n",
    "    def __init__(self, path: str) -> None:    \n",
    "        demand_path = path + \"/Demand_history.csv\"\n",
    "        existingEV_path = path + \"/existing_EV_infrastructure_2018.csv\"\n",
    "        self.df_orig = pd.read_csv(demand_path)\n",
    "        self.df_trans = self.df_orig\n",
    "        self.years_window = 2\n",
    "        self.y_cols = [f\"n-{y}\" for y in range(1, self.years_window + 1)]\n",
    "        self.dem_scaler = MinMaxScaler()\n",
    "        self.coord_scaler = MinMaxScaler()\n",
    "        self.seq_len = 8\n",
    "        \n",
    "    def clean(self):\n",
    "        self.df_orig.loc[(self.df_orig != 0).any(1)]\n",
    "        print(self.df_orig)\n",
    "        \n",
    "    def process(self):\n",
    "        all_dem = self.df_trans[\n",
    "            self.df_trans.columns[self.df_trans.columns.str.startswith('20')]].stack()\n",
    "        self.dem_scaler.fit(all_dem.values.reshape(-1, 1))\n",
    "        self.coord_scaler.fit(self.df_trans.loc[:, self.df_trans.columns.str.contains('coord')])\n",
    "        \n",
    "        self.x_proc = pd.DataFrame(columns=[\"x_coordinate\", \"y_coordinate\", *self.y_cols])\n",
    "        self.y_proc = pd.Series(dtype=np.float64)\n",
    "        for y in self.df_trans.columns[self.df_trans.columns.str.startswith('20')]:\n",
    "            y = int(y)\n",
    "            if y < 2010 + self.years_window:\n",
    "                continue\n",
    "            y_cols = [f\"{y - i}\" for i in range(1, self.years_window + 1)]\n",
    "            stack_x = self.df_trans.loc[:, self.df_trans.columns.isin([\"x_coordinate\", \"y_coordinate\", *y_cols])]\n",
    "            stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = self.coord_scaler.transform(stack_x.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
    "            for col in y_cols:\n",
    "                stack_x.loc[:, col] = self.dem_scaler.transform(stack_x.loc[:, col].values.reshape(-1, 1))\n",
    "            y_dict = {f\"{y - i}\": f\"n-{i}\" for i in range(1, self.years_window + 1)}\n",
    "            stack_x = stack_x.rename(columns=y_dict)\n",
    "            stack_y = self.df_trans.loc[:, f\"{y}\"]\n",
    "            stack_y = pd.Series(self.dem_scaler.transform(stack_y.values.reshape(-1, 1)).flatten())\n",
    "            self.x_proc = pd.concat([self.x_proc, stack_x], axis=0, ignore_index=True)\n",
    "            self.y_proc = pd.concat([self.y_proc, stack_y], axis=0, ignore_index=True)\n",
    "        \n",
    "        self.x_list = [self.x_proc.iloc[i * self.seq_len: (i + 1) * self.seq_len, :] for i in range(int(self.x_proc.shape[0] / self.seq_len))]\n",
    "        self.y_list = [self.y_proc.iloc[i * self.seq_len: (i + 1) * self.seq_len] for i in range(int(self.x_proc.shape[0] / self.seq_len))]\n",
    "        # self.x_list = np.array(self.x_list)\n",
    "        # self.y_list = np.array(self.y_list)\n",
    "        self.train_idx = np.random.choice(len(self.x_list), int(len(self.x_list) * 0.8), replace=False)\n",
    "        self.test_idx = [i for i in range(len(self.x_list)) if i not in self.train_idx]   \n",
    "        \n",
    "        \n",
    "    def datagen(self, kind):\n",
    "        if kind == 'train':\n",
    "            idxs = self.train_idx\n",
    "        elif kind == 'valid':\n",
    "            idxs = self.test_idx\n",
    "            \n",
    "        x = [self.x_list[i].values for i in idxs] \n",
    "        y = [self.y_list[i].values for i in idxs] \n",
    "        return np.array(x), np.array(y)\n",
    "                    \n",
    "    def addYearDemandfromForecast(self, year: int, predicted: np.array) -> None:\n",
    "        predicted = predicted.flatten().reshape(-1, 1)\n",
    "        predicted = self.dem_scaler.inverse_transform(predicted)\n",
    "        self.df_trans[f\"{year}\"] = abs(predicted)        \n",
    "                  \n",
    "path = \"data\"\n",
    "data = Data(path)\n",
    "data.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, data: Data) -> None:\n",
    "        self.data = data\n",
    "        self.batch_size = 16\n",
    "        self.filter_size = 64\n",
    "        self.epochs = 1000\n",
    "        self.regularizer = k.regularizers.L1L2(l1=0, l2=0)\n",
    "    \n",
    "    def createModel(self):\n",
    "        inputs = l.Input(shape=(data.seq_len, data.years_window + 2))\n",
    "        cnn1 = l.Conv1D(self.filter_size, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(inputs)\n",
    "        cnn2 = l.Conv1D(self.filter_size, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(cnn1)\n",
    "        mp1 = l.MaxPool1D(pool_size=2)(cnn2)\n",
    "        cnn3 = l.Conv1D(self.filter_size * 2, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(mp1)\n",
    "        cnn4 = l.Conv1D(self.filter_size * 2, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(cnn3)\n",
    "        cnn5 = l.Conv1D(self.filter_size * 2, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(cnn4)\n",
    "        mp1 = l.MaxPool1D(pool_size=2)(cnn5)\n",
    "        cnn3 = l.Conv1D(self.filter_size * 4, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(mp1)\n",
    "        cnn4 = l.Conv1D(self.filter_size * 4, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(cnn3)\n",
    "        cnn5 = l.Conv1D(self.filter_size * 4, kernel_size=3, activation=\"relu\", padding=\"same\",\n",
    "                        kernel_regularizer=self.regularizer)(cnn4)\n",
    "        mp2 = l.MaxPool1D(pool_size=2)(cnn5)\n",
    "        fl = l.Flatten()(mp2)\n",
    "        do = l.Dropout(0.1)(fl)\n",
    "        outputs = l.Dense(data.seq_len, activation=\"relu\")(do)\n",
    "        self.model = k.Model(inputs=inputs, outputs=outputs)\n",
    "        self.model.compile(optimizer=k.optimizers.Adam(learning_rate=1e-3), loss='mse')\n",
    "        self.model.summary()\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        \n",
    "        callbacks = []\n",
    "        # mp = \"/mod/checkpoint\"\n",
    "        # cbcp = k.ModelCheckpoint(mp,\n",
    "        #             monitor='val_mse', mode=\"auto\", verbose=0,\n",
    "        #             save_best_only=True, save_weights_only=True, save_freq=\"epoch\")\n",
    "        cbes = k.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0,\n",
    "            patience=100,\n",
    "            verbose=0,\n",
    "            mode=\"auto\",\n",
    "            baseline=None,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        callbacks.append(cbes)\n",
    "        \n",
    "        rlr = k.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=20,\n",
    "            min_lr=0,\n",
    "            min_delta=0.00008)\n",
    "        \n",
    "        callbacks.append(rlr)\n",
    "        \n",
    "        \n",
    "        x_t, y_t = data.datagen('train')\n",
    "        x_v, y_v = data.datagen('valid')\n",
    "        \n",
    "        self.history = self.model.fit(x_t, y_t, validation_data=(x_v, y_v),\n",
    "                                      epochs=self.epochs, verbose=1, callbacks=callbacks)\n",
    "        # self.model.load_weights(mp) \n",
    "        \n",
    "        pred = self.model.predict(x_t)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_t, pred))\n",
    "        train_mae = mean_absolute_error(y_t, pred)   \n",
    "        train_r2 = r2_score(y_t, pred) \n",
    "        pred = self.model.predict(x_v)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_v, pred))\n",
    "        test_mae = mean_absolute_error(y_v, pred)   \n",
    "        test_r2 = r2_score(y_v, pred) \n",
    "           \n",
    "        \n",
    "        print(f\"training: rmse={train_rmse}, mae={train_mae}, r2={train_r2}\")   \n",
    "        print(f\"test: rmse={test_rmse}, mae={test_mae}, r2={test_r2}\")                        \n",
    "    \n",
    "    def predict(self, year: int) -> pd.Series:\n",
    "        y_cols = [f\"{year - i}\" for i in range(1, data.years_window + 1)]\n",
    "        x_forecast = \\\n",
    "            data.df_trans.loc[:,\n",
    "                              data.df_trans.columns.isin([\"x_coordinate\", \"y_coordinate\", *y_cols])]        \n",
    "        x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = data.coord_scaler.transform(x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
    "        for col in y_cols:  \n",
    "            x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
    "        x_list = [x_forecast.iloc[i * data.seq_len: (i + 1) * data.seq_len, :] for i in range(int(x_forecast.shape[0] / data.seq_len))]\n",
    "        x_forecast = np.array(x_list)\n",
    "        print(x_forecast.shape)\n",
    "        return self.model.predict(x_forecast)\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        params['n_estimators'] = int(params['n_estimators'])\n",
    "        params['max_depth'] = int(params['max_depth'])\n",
    "        self.model.set_params(**params)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 8, 4)]            0         \n",
      "                                                                 \n",
      " conv1d_38 (Conv1D)          (None, 8, 64)             832       \n",
      "                                                                 \n",
      " conv1d_39 (Conv1D)          (None, 8, 64)             12352     \n",
      "                                                                 \n",
      " max_pooling1d_17 (MaxPoolin  (None, 4, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_40 (Conv1D)          (None, 4, 128)            24704     \n",
      "                                                                 \n",
      " conv1d_41 (Conv1D)          (None, 4, 128)            49280     \n",
      "                                                                 \n",
      " conv1d_42 (Conv1D)          (None, 4, 128)            49280     \n",
      "                                                                 \n",
      " max_pooling1d_18 (MaxPoolin  (None, 2, 128)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_43 (Conv1D)          (None, 2, 256)            98560     \n",
      "                                                                 \n",
      " conv1d_44 (Conv1D)          (None, 2, 256)            196864    \n",
      "                                                                 \n",
      " conv1d_45 (Conv1D)          (None, 2, 256)            196864    \n",
      "                                                                 \n",
      " max_pooling1d_19 (MaxPoolin  (None, 1, 256)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 630,792\n",
      "Trainable params: 630,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.createModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:45:39.000574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.0109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:45:41.049389: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 3s 19ms/step - loss: 0.0109 - val_loss: 0.0045 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0047 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0037 - val_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0030 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0026 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0023 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0024 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0025 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0021 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0022 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0022 - val_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0020 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0020 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0024 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0017 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0017 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0017 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0016 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0016 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0015 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0015 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0014 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0013 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0013 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0014 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0012 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0012 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0011 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0012 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0010 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0011 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.9316e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0013 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0011 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 0.0010 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.1216e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.3826e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 0.0010 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.5445e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 8.3430e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 7.9965e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 7.8637e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 7.9845e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 7.8298e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 8.5788e-04 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 8.9493e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 7.4712e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 8.6738e-04 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 7.5828e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 5.5125e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 56/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.9448e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 57/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 4.7445e-04 - val_loss: 9.6039e-04 - lr: 5.0000e-04\n",
      "Epoch 58/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.7152e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 59/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.7125e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 60/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.5135e-04 - val_loss: 9.8966e-04 - lr: 5.0000e-04\n",
      "Epoch 61/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.5977e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 62/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.8329e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 63/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 5.0408e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 64/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.4486e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 65/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.2968e-04 - val_loss: 9.7955e-04 - lr: 5.0000e-04\n",
      "Epoch 66/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.3186e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 67/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.0748e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 68/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.1911e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 69/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.0151e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 70/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.0830e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 71/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.0478e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 72/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 4.1591e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 73/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 3.9239e-04 - val_loss: 0.0010 - lr: 5.0000e-04\n",
      "Epoch 74/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 3.7183e-04 - val_loss: 9.9600e-04 - lr: 5.0000e-04\n",
      "Epoch 75/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 3.5857e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 76/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.9701e-04 - val_loss: 9.6620e-04 - lr: 2.5000e-04\n",
      "Epoch 77/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 2.7680e-04 - val_loss: 9.6855e-04 - lr: 2.5000e-04\n",
      "Epoch 78/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 2.7063e-04 - val_loss: 9.4924e-04 - lr: 2.5000e-04\n",
      "Epoch 79/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 2.5989e-04 - val_loss: 9.6434e-04 - lr: 2.5000e-04\n",
      "Epoch 80/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.5814e-04 - val_loss: 9.6211e-04 - lr: 2.5000e-04\n",
      "Epoch 81/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.6315e-04 - val_loss: 0.0010 - lr: 2.5000e-04\n",
      "Epoch 82/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.5544e-04 - val_loss: 9.9326e-04 - lr: 2.5000e-04\n",
      "Epoch 83/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.5081e-04 - val_loss: 9.7396e-04 - lr: 2.5000e-04\n",
      "Epoch 84/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.4371e-04 - val_loss: 9.9810e-04 - lr: 2.5000e-04\n",
      "Epoch 85/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.4489e-04 - val_loss: 9.8726e-04 - lr: 2.5000e-04\n",
      "Epoch 86/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.4365e-04 - val_loss: 0.0010 - lr: 2.5000e-04\n",
      "Epoch 87/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.4403e-04 - val_loss: 9.9279e-04 - lr: 2.5000e-04\n",
      "Epoch 88/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.3499e-04 - val_loss: 9.6219e-04 - lr: 2.5000e-04\n",
      "Epoch 89/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.3540e-04 - val_loss: 9.7202e-04 - lr: 2.5000e-04\n",
      "Epoch 90/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.3085e-04 - val_loss: 0.0011 - lr: 2.5000e-04\n",
      "Epoch 91/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.3428e-04 - val_loss: 9.7274e-04 - lr: 2.5000e-04\n",
      "Epoch 92/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.2524e-04 - val_loss: 9.5856e-04 - lr: 2.5000e-04\n",
      "Epoch 93/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 2.3211e-04 - val_loss: 9.8599e-04 - lr: 2.5000e-04\n",
      "Epoch 94/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 2.1985e-04 - val_loss: 9.8989e-04 - lr: 2.5000e-04\n",
      "Epoch 95/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 2.2210e-04 - val_loss: 9.7123e-04 - lr: 2.5000e-04\n",
      "Epoch 96/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.9116e-04 - val_loss: 9.5807e-04 - lr: 1.2500e-04\n",
      "Epoch 97/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.7772e-04 - val_loss: 9.5764e-04 - lr: 1.2500e-04\n",
      "Epoch 98/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.7564e-04 - val_loss: 9.6143e-04 - lr: 1.2500e-04\n",
      "Epoch 99/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.7369e-04 - val_loss: 9.5163e-04 - lr: 1.2500e-04\n",
      "Epoch 100/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.7176e-04 - val_loss: 9.6152e-04 - lr: 1.2500e-04\n",
      "Epoch 101/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 1.6804e-04 - val_loss: 9.8475e-04 - lr: 1.2500e-04\n",
      "Epoch 102/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.6809e-04 - val_loss: 9.8703e-04 - lr: 1.2500e-04\n",
      "Epoch 103/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.6788e-04 - val_loss: 9.5331e-04 - lr: 1.2500e-04\n",
      "Epoch 104/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.6632e-04 - val_loss: 9.4781e-04 - lr: 1.2500e-04\n",
      "Epoch 105/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.6607e-04 - val_loss: 9.8100e-04 - lr: 1.2500e-04\n",
      "Epoch 106/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.6411e-04 - val_loss: 9.8047e-04 - lr: 1.2500e-04\n",
      "Epoch 107/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5870e-04 - val_loss: 9.6408e-04 - lr: 1.2500e-04\n",
      "Epoch 108/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5884e-04 - val_loss: 9.8333e-04 - lr: 1.2500e-04\n",
      "Epoch 109/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5973e-04 - val_loss: 9.7284e-04 - lr: 1.2500e-04\n",
      "Epoch 110/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5823e-04 - val_loss: 9.7920e-04 - lr: 1.2500e-04\n",
      "Epoch 111/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5960e-04 - val_loss: 9.8517e-04 - lr: 1.2500e-04\n",
      "Epoch 112/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5370e-04 - val_loss: 9.9265e-04 - lr: 1.2500e-04\n",
      "Epoch 113/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5245e-04 - val_loss: 9.8283e-04 - lr: 1.2500e-04\n",
      "Epoch 114/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.5153e-04 - val_loss: 9.6919e-04 - lr: 1.2500e-04\n",
      "Epoch 115/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.4905e-04 - val_loss: 9.6951e-04 - lr: 1.2500e-04\n",
      "Epoch 116/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3875e-04 - val_loss: 9.6041e-04 - lr: 6.2500e-05\n",
      "Epoch 117/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3780e-04 - val_loss: 9.9260e-04 - lr: 6.2500e-05\n",
      "Epoch 118/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3400e-04 - val_loss: 9.6241e-04 - lr: 6.2500e-05\n",
      "Epoch 119/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3120e-04 - val_loss: 9.6865e-04 - lr: 6.2500e-05\n",
      "Epoch 120/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3200e-04 - val_loss: 9.7128e-04 - lr: 6.2500e-05\n",
      "Epoch 121/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3204e-04 - val_loss: 9.7518e-04 - lr: 6.2500e-05\n",
      "Epoch 122/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3116e-04 - val_loss: 9.7360e-04 - lr: 6.2500e-05\n",
      "Epoch 123/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3062e-04 - val_loss: 9.6978e-04 - lr: 6.2500e-05\n",
      "Epoch 124/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.3231e-04 - val_loss: 9.9384e-04 - lr: 6.2500e-05\n",
      "Epoch 125/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2997e-04 - val_loss: 9.6970e-04 - lr: 6.2500e-05\n",
      "Epoch 126/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2775e-04 - val_loss: 9.7893e-04 - lr: 6.2500e-05\n",
      "Epoch 127/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2904e-04 - val_loss: 9.7662e-04 - lr: 6.2500e-05\n",
      "Epoch 128/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2655e-04 - val_loss: 9.7847e-04 - lr: 6.2500e-05\n",
      "Epoch 129/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2879e-04 - val_loss: 9.7805e-04 - lr: 6.2500e-05\n",
      "Epoch 130/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2580e-04 - val_loss: 9.7358e-04 - lr: 6.2500e-05\n",
      "Epoch 131/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2538e-04 - val_loss: 9.8463e-04 - lr: 6.2500e-05\n",
      "Epoch 132/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2396e-04 - val_loss: 9.7998e-04 - lr: 6.2500e-05\n",
      "Epoch 133/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2592e-04 - val_loss: 9.7500e-04 - lr: 6.2500e-05\n",
      "Epoch 134/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2435e-04 - val_loss: 9.8480e-04 - lr: 6.2500e-05\n",
      "Epoch 135/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.2279e-04 - val_loss: 9.7583e-04 - lr: 6.2500e-05\n",
      "Epoch 136/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 1.1708e-04 - val_loss: 9.8364e-04 - lr: 3.1250e-05\n",
      "Epoch 137/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1533e-04 - val_loss: 9.9071e-04 - lr: 3.1250e-05\n",
      "Epoch 138/1000\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 1.1532e-04 - val_loss: 9.7571e-04 - lr: 3.1250e-05\n",
      "Epoch 139/1000\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 1.1482e-04 - val_loss: 9.7975e-04 - lr: 3.1250e-05\n",
      "Epoch 140/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1521e-04 - val_loss: 9.7873e-04 - lr: 3.1250e-05\n",
      "Epoch 141/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1577e-04 - val_loss: 9.7101e-04 - lr: 3.1250e-05\n",
      "Epoch 142/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1316e-04 - val_loss: 9.7513e-04 - lr: 3.1250e-05\n",
      "Epoch 143/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1289e-04 - val_loss: 9.7941e-04 - lr: 3.1250e-05\n",
      "Epoch 144/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1378e-04 - val_loss: 9.7266e-04 - lr: 3.1250e-05\n",
      "Epoch 145/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1292e-04 - val_loss: 9.7379e-04 - lr: 3.1250e-05\n",
      "Epoch 146/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 1.1211e-04 - val_loss: 9.8032e-04 - lr: 3.1250e-05\n",
      "Epoch 147/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 1.0960e-04 - val_loss: 9.8206e-04 - lr: 3.1250e-05\n",
      "Epoch 148/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1084e-04 - val_loss: 9.8392e-04 - lr: 3.1250e-05\n",
      "Epoch 149/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1153e-04 - val_loss: 9.8342e-04 - lr: 3.1250e-05\n",
      "Epoch 150/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1101e-04 - val_loss: 9.7213e-04 - lr: 3.1250e-05\n",
      "Epoch 151/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1205e-04 - val_loss: 9.8217e-04 - lr: 3.1250e-05\n",
      "Epoch 152/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.1101e-04 - val_loss: 9.7795e-04 - lr: 3.1250e-05\n",
      "Epoch 153/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0908e-04 - val_loss: 9.7981e-04 - lr: 3.1250e-05\n",
      "Epoch 154/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0889e-04 - val_loss: 9.8212e-04 - lr: 3.1250e-05\n",
      "Epoch 155/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0728e-04 - val_loss: 9.7427e-04 - lr: 3.1250e-05\n",
      "Epoch 156/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0585e-04 - val_loss: 9.7022e-04 - lr: 1.5625e-05\n",
      "Epoch 157/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0640e-04 - val_loss: 9.7627e-04 - lr: 1.5625e-05\n",
      "Epoch 158/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0429e-04 - val_loss: 9.7777e-04 - lr: 1.5625e-05\n",
      "Epoch 159/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0436e-04 - val_loss: 9.8298e-04 - lr: 1.5625e-05\n",
      "Epoch 160/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0433e-04 - val_loss: 9.8529e-04 - lr: 1.5625e-05\n",
      "Epoch 161/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0399e-04 - val_loss: 9.7525e-04 - lr: 1.5625e-05\n",
      "Epoch 162/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0431e-04 - val_loss: 9.7505e-04 - lr: 1.5625e-05\n",
      "Epoch 163/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0396e-04 - val_loss: 9.7706e-04 - lr: 1.5625e-05\n",
      "Epoch 164/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0337e-04 - val_loss: 9.7791e-04 - lr: 1.5625e-05\n",
      "Epoch 165/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0495e-04 - val_loss: 9.8409e-04 - lr: 1.5625e-05\n",
      "Epoch 166/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0385e-04 - val_loss: 9.8093e-04 - lr: 1.5625e-05\n",
      "Epoch 167/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0414e-04 - val_loss: 9.7698e-04 - lr: 1.5625e-05\n",
      "Epoch 168/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0244e-04 - val_loss: 9.8058e-04 - lr: 1.5625e-05\n",
      "Epoch 169/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0460e-04 - val_loss: 9.8184e-04 - lr: 1.5625e-05\n",
      "Epoch 170/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0247e-04 - val_loss: 9.8218e-04 - lr: 1.5625e-05\n",
      "Epoch 171/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0339e-04 - val_loss: 9.8151e-04 - lr: 1.5625e-05\n",
      "Epoch 172/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0303e-04 - val_loss: 9.8562e-04 - lr: 1.5625e-05\n",
      "Epoch 173/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0328e-04 - val_loss: 9.8022e-04 - lr: 1.5625e-05\n",
      "Epoch 174/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0207e-04 - val_loss: 9.8106e-04 - lr: 1.5625e-05\n",
      "Epoch 175/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0308e-04 - val_loss: 9.8305e-04 - lr: 1.5625e-05\n",
      "Epoch 176/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0206e-04 - val_loss: 9.7700e-04 - lr: 7.8125e-06\n",
      "Epoch 177/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0202e-04 - val_loss: 9.7887e-04 - lr: 7.8125e-06\n",
      "Epoch 178/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0180e-04 - val_loss: 9.7990e-04 - lr: 7.8125e-06\n",
      "Epoch 179/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0127e-04 - val_loss: 9.7447e-04 - lr: 7.8125e-06\n",
      "Epoch 180/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0066e-04 - val_loss: 9.7894e-04 - lr: 7.8125e-06\n",
      "Epoch 181/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0003e-04 - val_loss: 9.7867e-04 - lr: 7.8125e-06\n",
      "Epoch 182/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0009e-04 - val_loss: 9.8244e-04 - lr: 7.8125e-06\n",
      "Epoch 183/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.9896e-05 - val_loss: 9.8331e-04 - lr: 7.8125e-06\n",
      "Epoch 184/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.9690e-05 - val_loss: 9.8270e-04 - lr: 7.8125e-06\n",
      "Epoch 185/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0061e-04 - val_loss: 9.7849e-04 - lr: 7.8125e-06\n",
      "Epoch 186/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.9846e-05 - val_loss: 9.8207e-04 - lr: 7.8125e-06\n",
      "Epoch 187/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0005e-04 - val_loss: 9.7981e-04 - lr: 7.8125e-06\n",
      "Epoch 188/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0143e-04 - val_loss: 9.7935e-04 - lr: 7.8125e-06\n",
      "Epoch 189/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8066e-05 - val_loss: 9.7947e-04 - lr: 7.8125e-06\n",
      "Epoch 190/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0020e-04 - val_loss: 9.7476e-04 - lr: 7.8125e-06\n",
      "Epoch 191/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8648e-05 - val_loss: 9.8494e-04 - lr: 7.8125e-06\n",
      "Epoch 192/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8911e-05 - val_loss: 9.7928e-04 - lr: 7.8125e-06\n",
      "Epoch 193/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0019e-04 - val_loss: 9.8038e-04 - lr: 7.8125e-06\n",
      "Epoch 194/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8111e-05 - val_loss: 9.8222e-04 - lr: 7.8125e-06\n",
      "Epoch 195/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 1.0046e-04 - val_loss: 9.8321e-04 - lr: 7.8125e-06\n",
      "Epoch 196/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 9.7680e-05 - val_loss: 9.8009e-04 - lr: 3.9063e-06\n",
      "Epoch 197/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8358e-05 - val_loss: 9.8282e-04 - lr: 3.9063e-06\n",
      "Epoch 198/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 9.8504e-05 - val_loss: 9.8234e-04 - lr: 3.9063e-06\n",
      "Epoch 199/1000\n",
      "90/90 [==============================] - 1s 14ms/step - loss: 9.7258e-05 - val_loss: 9.8061e-04 - lr: 3.9063e-06\n",
      "Epoch 200/1000\n",
      "90/90 [==============================] - 1s 15ms/step - loss: 9.9137e-05 - val_loss: 9.7983e-04 - lr: 3.9063e-06\n",
      "Epoch 201/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8433e-05 - val_loss: 9.7940e-04 - lr: 3.9063e-06\n",
      "Epoch 202/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.7919e-05 - val_loss: 9.8345e-04 - lr: 3.9063e-06\n",
      "Epoch 203/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.7715e-05 - val_loss: 9.8259e-04 - lr: 3.9063e-06\n",
      "Epoch 204/1000\n",
      "90/90 [==============================] - 1s 13ms/step - loss: 9.8148e-05 - val_loss: 9.8214e-04 - lr: 3.9063e-06\n",
      "14/90 [===>..........................] - ETA: 0s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-16 10:49:41.113628: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1s 4ms/step\n",
      "23/23 [==============================] - 0s 6ms/step\n",
      "training: rmse=0.0118104977384521, mae=0.007737636392221938, r2=0.9958170838013294\n",
      "test: rmse=0.030786544355882788, mae=0.01741299123442782, r2=0.9710634096168204\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 8, 4)\n",
      "16/16 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/4252624017.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = data.coord_scaler.transform(x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/4252624017.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/4252624017.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "predicted_2019 = model.predict(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.addYearDemandfromForecast(2019, predicted_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_point_index</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.352242</td>\n",
       "      <td>0.667932</td>\n",
       "      <td>0.958593</td>\n",
       "      <td>2.911901</td>\n",
       "      <td>4.338274</td>\n",
       "      <td>6.561995</td>\n",
       "      <td>8.454417</td>\n",
       "      <td>10.595324</td>\n",
       "      <td>13.119572</td>\n",
       "      <td>16.135149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.325940</td>\n",
       "      <td>0.591964</td>\n",
       "      <td>0.862652</td>\n",
       "      <td>2.589068</td>\n",
       "      <td>4.196034</td>\n",
       "      <td>5.745551</td>\n",
       "      <td>8.753195</td>\n",
       "      <td>11.126995</td>\n",
       "      <td>12.020091</td>\n",
       "      <td>15.068899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.373752</td>\n",
       "      <td>0.591890</td>\n",
       "      <td>0.969733</td>\n",
       "      <td>2.641432</td>\n",
       "      <td>3.541772</td>\n",
       "      <td>5.469161</td>\n",
       "      <td>8.414627</td>\n",
       "      <td>10.115336</td>\n",
       "      <td>14.018254</td>\n",
       "      <td>15.171862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.420686</td>\n",
       "      <td>0.584055</td>\n",
       "      <td>0.906547</td>\n",
       "      <td>2.378577</td>\n",
       "      <td>3.888121</td>\n",
       "      <td>5.846089</td>\n",
       "      <td>9.083868</td>\n",
       "      <td>12.424885</td>\n",
       "      <td>15.012302</td>\n",
       "      <td>16.787079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.475621</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.981544</td>\n",
       "      <td>2.665400</td>\n",
       "      <td>4.218711</td>\n",
       "      <td>6.776609</td>\n",
       "      <td>8.851107</td>\n",
       "      <td>11.731131</td>\n",
       "      <td>16.355563</td>\n",
       "      <td>17.157883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>4091</td>\n",
       "      <td>59.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.334565</td>\n",
       "      <td>0.556055</td>\n",
       "      <td>1.373291</td>\n",
       "      <td>1.837586</td>\n",
       "      <td>2.517146</td>\n",
       "      <td>3.352280</td>\n",
       "      <td>4.149888</td>\n",
       "      <td>5.426193</td>\n",
       "      <td>7.562072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4092</td>\n",
       "      <td>60.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.041716</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.131291</td>\n",
       "      <td>0.386540</td>\n",
       "      <td>0.755846</td>\n",
       "      <td>0.941116</td>\n",
       "      <td>1.107797</td>\n",
       "      <td>1.309479</td>\n",
       "      <td>2.057450</td>\n",
       "      <td>3.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>4093</td>\n",
       "      <td>61.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.100895</td>\n",
       "      <td>0.180352</td>\n",
       "      <td>0.296299</td>\n",
       "      <td>0.705373</td>\n",
       "      <td>1.300220</td>\n",
       "      <td>1.608609</td>\n",
       "      <td>1.822806</td>\n",
       "      <td>2.333681</td>\n",
       "      <td>3.218519</td>\n",
       "      <td>3.645210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>4094</td>\n",
       "      <td>62.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.155353</td>\n",
       "      <td>0.290825</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>1.516066</td>\n",
       "      <td>2.399426</td>\n",
       "      <td>2.719197</td>\n",
       "      <td>4.494515</td>\n",
       "      <td>6.096858</td>\n",
       "      <td>6.262574</td>\n",
       "      <td>11.451424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>4095</td>\n",
       "      <td>63.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.340185</td>\n",
       "      <td>0.749491</td>\n",
       "      <td>1.904285</td>\n",
       "      <td>2.775772</td>\n",
       "      <td>3.404641</td>\n",
       "      <td>4.574922</td>\n",
       "      <td>6.301078</td>\n",
       "      <td>6.860939</td>\n",
       "      <td>11.221158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      demand_point_index  x_coordinate  y_coordinate      2010      2011  \\\n",
       "0                      0           0.5           0.5  0.352242  0.667932   \n",
       "1                      1           1.5           0.5  0.325940  0.591964   \n",
       "2                      2           2.5           0.5  0.373752  0.591890   \n",
       "3                      3           3.5           0.5  0.420686  0.584055   \n",
       "4                      4           4.5           0.5  0.475621  0.647940   \n",
       "...                  ...           ...           ...       ...       ...   \n",
       "4091                4091          59.5          63.5  0.171015  0.334565   \n",
       "4092                4092          60.5          63.5  0.041716  0.061741   \n",
       "4093                4093          61.5          63.5  0.100895  0.180352   \n",
       "4094                4094          62.5          63.5  0.155353  0.290825   \n",
       "4095                4095          63.5          63.5  0.209372  0.340185   \n",
       "\n",
       "          2012      2013      2014      2015      2016       2017       2018  \\\n",
       "0     0.958593  2.911901  4.338274  6.561995  8.454417  10.595324  13.119572   \n",
       "1     0.862652  2.589068  4.196034  5.745551  8.753195  11.126995  12.020091   \n",
       "2     0.969733  2.641432  3.541772  5.469161  8.414627  10.115336  14.018254   \n",
       "3     0.906547  2.378577  3.888121  5.846089  9.083868  12.424885  15.012302   \n",
       "4     0.981544  2.665400  4.218711  6.776609  8.851107  11.731131  16.355563   \n",
       "...        ...       ...       ...       ...       ...        ...        ...   \n",
       "4091  0.556055  1.373291  1.837586  2.517146  3.352280   4.149888   5.426193   \n",
       "4092  0.131291  0.386540  0.755846  0.941116  1.107797   1.309479   2.057450   \n",
       "4093  0.296299  0.705373  1.300220  1.608609  1.822806   2.333681   3.218519   \n",
       "4094  0.557803  1.516066  2.399426  2.719197  4.494515   6.096858   6.262574   \n",
       "4095  0.749491  1.904285  2.775772  3.404641  4.574922   6.301078   6.860939   \n",
       "\n",
       "           2019  \n",
       "0     16.135149  \n",
       "1     15.068899  \n",
       "2     15.171862  \n",
       "3     16.787079  \n",
       "4     17.157883  \n",
       "...         ...  \n",
       "4091   7.562072  \n",
       "4092   3.184400  \n",
       "4093   3.645210  \n",
       "4094  11.451424  \n",
       "4095  11.221158  \n",
       "\n",
       "[4096 rows x 13 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 8, 4)\n",
      "16/16 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/4252624017.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]] = data.coord_scaler.transform(x_forecast.loc[:, [\"x_coordinate\", \"y_coordinate\"]])\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/4252624017.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n",
      "/var/folders/fw/73064vz92b50801nky3xxl0r0000gp/T/ipykernel_6156/4252624017.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_forecast.loc[:, col] = data.dem_scaler.transform(x_forecast.loc[:, col].values.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "predicted_2020 = model.predict(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.addYearDemandfromForecast(2020, predicted_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_point_index</th>\n",
       "      <th>x_coordinate</th>\n",
       "      <th>y_coordinate</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.352242</td>\n",
       "      <td>0.667932</td>\n",
       "      <td>0.958593</td>\n",
       "      <td>2.911901</td>\n",
       "      <td>4.338274</td>\n",
       "      <td>6.561995</td>\n",
       "      <td>8.454417</td>\n",
       "      <td>10.595324</td>\n",
       "      <td>13.119572</td>\n",
       "      <td>16.135149</td>\n",
       "      <td>19.231125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.325940</td>\n",
       "      <td>0.591964</td>\n",
       "      <td>0.862652</td>\n",
       "      <td>2.589068</td>\n",
       "      <td>4.196034</td>\n",
       "      <td>5.745551</td>\n",
       "      <td>8.753195</td>\n",
       "      <td>11.126995</td>\n",
       "      <td>12.020091</td>\n",
       "      <td>15.068899</td>\n",
       "      <td>17.595203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.373752</td>\n",
       "      <td>0.591890</td>\n",
       "      <td>0.969733</td>\n",
       "      <td>2.641432</td>\n",
       "      <td>3.541772</td>\n",
       "      <td>5.469161</td>\n",
       "      <td>8.414627</td>\n",
       "      <td>10.115336</td>\n",
       "      <td>14.018254</td>\n",
       "      <td>15.171862</td>\n",
       "      <td>17.007502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.420686</td>\n",
       "      <td>0.584055</td>\n",
       "      <td>0.906547</td>\n",
       "      <td>2.378577</td>\n",
       "      <td>3.888121</td>\n",
       "      <td>5.846089</td>\n",
       "      <td>9.083868</td>\n",
       "      <td>12.424885</td>\n",
       "      <td>15.012302</td>\n",
       "      <td>16.787079</td>\n",
       "      <td>18.480434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.475621</td>\n",
       "      <td>0.647940</td>\n",
       "      <td>0.981544</td>\n",
       "      <td>2.665400</td>\n",
       "      <td>4.218711</td>\n",
       "      <td>6.776609</td>\n",
       "      <td>8.851107</td>\n",
       "      <td>11.731131</td>\n",
       "      <td>16.355563</td>\n",
       "      <td>17.157883</td>\n",
       "      <td>18.493801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>4091</td>\n",
       "      <td>59.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.334565</td>\n",
       "      <td>0.556055</td>\n",
       "      <td>1.373291</td>\n",
       "      <td>1.837586</td>\n",
       "      <td>2.517146</td>\n",
       "      <td>3.352280</td>\n",
       "      <td>4.149888</td>\n",
       "      <td>5.426193</td>\n",
       "      <td>7.562072</td>\n",
       "      <td>11.123329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>4092</td>\n",
       "      <td>60.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.041716</td>\n",
       "      <td>0.061741</td>\n",
       "      <td>0.131291</td>\n",
       "      <td>0.386540</td>\n",
       "      <td>0.755846</td>\n",
       "      <td>0.941116</td>\n",
       "      <td>1.107797</td>\n",
       "      <td>1.309479</td>\n",
       "      <td>2.057450</td>\n",
       "      <td>3.184400</td>\n",
       "      <td>4.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>4093</td>\n",
       "      <td>61.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.100895</td>\n",
       "      <td>0.180352</td>\n",
       "      <td>0.296299</td>\n",
       "      <td>0.705373</td>\n",
       "      <td>1.300220</td>\n",
       "      <td>1.608609</td>\n",
       "      <td>1.822806</td>\n",
       "      <td>2.333681</td>\n",
       "      <td>3.218519</td>\n",
       "      <td>3.645210</td>\n",
       "      <td>5.762763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>4094</td>\n",
       "      <td>62.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.155353</td>\n",
       "      <td>0.290825</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>1.516066</td>\n",
       "      <td>2.399426</td>\n",
       "      <td>2.719197</td>\n",
       "      <td>4.494515</td>\n",
       "      <td>6.096858</td>\n",
       "      <td>6.262574</td>\n",
       "      <td>11.451424</td>\n",
       "      <td>17.879885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>4095</td>\n",
       "      <td>63.5</td>\n",
       "      <td>63.5</td>\n",
       "      <td>0.209372</td>\n",
       "      <td>0.340185</td>\n",
       "      <td>0.749491</td>\n",
       "      <td>1.904285</td>\n",
       "      <td>2.775772</td>\n",
       "      <td>3.404641</td>\n",
       "      <td>4.574922</td>\n",
       "      <td>6.301078</td>\n",
       "      <td>6.860939</td>\n",
       "      <td>11.221158</td>\n",
       "      <td>15.960327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4096 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      demand_point_index  x_coordinate  y_coordinate      2010      2011  \\\n",
       "0                      0           0.5           0.5  0.352242  0.667932   \n",
       "1                      1           1.5           0.5  0.325940  0.591964   \n",
       "2                      2           2.5           0.5  0.373752  0.591890   \n",
       "3                      3           3.5           0.5  0.420686  0.584055   \n",
       "4                      4           4.5           0.5  0.475621  0.647940   \n",
       "...                  ...           ...           ...       ...       ...   \n",
       "4091                4091          59.5          63.5  0.171015  0.334565   \n",
       "4092                4092          60.5          63.5  0.041716  0.061741   \n",
       "4093                4093          61.5          63.5  0.100895  0.180352   \n",
       "4094                4094          62.5          63.5  0.155353  0.290825   \n",
       "4095                4095          63.5          63.5  0.209372  0.340185   \n",
       "\n",
       "          2012      2013      2014      2015      2016       2017       2018  \\\n",
       "0     0.958593  2.911901  4.338274  6.561995  8.454417  10.595324  13.119572   \n",
       "1     0.862652  2.589068  4.196034  5.745551  8.753195  11.126995  12.020091   \n",
       "2     0.969733  2.641432  3.541772  5.469161  8.414627  10.115336  14.018254   \n",
       "3     0.906547  2.378577  3.888121  5.846089  9.083868  12.424885  15.012302   \n",
       "4     0.981544  2.665400  4.218711  6.776609  8.851107  11.731131  16.355563   \n",
       "...        ...       ...       ...       ...       ...        ...        ...   \n",
       "4091  0.556055  1.373291  1.837586  2.517146  3.352280   4.149888   5.426193   \n",
       "4092  0.131291  0.386540  0.755846  0.941116  1.107797   1.309479   2.057450   \n",
       "4093  0.296299  0.705373  1.300220  1.608609  1.822806   2.333681   3.218519   \n",
       "4094  0.557803  1.516066  2.399426  2.719197  4.494515   6.096858   6.262574   \n",
       "4095  0.749491  1.904285  2.775772  3.404641  4.574922   6.301078   6.860939   \n",
       "\n",
       "           2019       2020  \n",
       "0     16.135149  19.231125  \n",
       "1     15.068899  17.595203  \n",
       "2     15.171862  17.007502  \n",
       "3     16.787079  18.480434  \n",
       "4     17.157883  18.493801  \n",
       "...         ...        ...  \n",
       "4091   7.562072  11.123329  \n",
       "4092   3.184400   4.729700  \n",
       "4093   3.645210   5.762763  \n",
       "4094  11.451424  17.879885  \n",
       "4095  11.221158  15.960327  \n",
       "\n",
       "[4096 rows x 14 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.df_trans.to_csv(\"data/forecast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f414c7ff5a53ed42f5544c97695d2b1ab51d06b1c0aa8a68a2484fa0fe1f8ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
